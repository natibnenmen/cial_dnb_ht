# Data Extractor

## Usage
There are two ways to run the script, either running natively on the host, or running a docker container
### Running natively on the host
1. Clone  the following git repo:

    ```git@github.com:natibnenmen/cial_dnb_ht.git```

2. On the root directory, run:

    ```python3 process.py <zip file>```

    Where the ```<zip file>``` is the input zip file which contains the 'debts.txt' input file.

### Running a Docker container
Here again, there are two ways to run the docker image. You can either pull the image from docker hub, or build the image by yourself
1. Pulling the image from docker hub

    Run the following command:

    ```docker run --rm -v <host dir>:<container dir> --name <container name> natibenmen/dnb-data-extractor:1.0 python3 ./process.py <container dir>/<input zip file>```

    Where:

    ```<host dir>``` is a directory on the host which contains the ```<input zip file>```. Note that is should be a full path, not a ralative one to the current directory.

    ```<container dir>``` is the mount point within the container.

    ```<container name>``` is whatever name you give your container

    e.g.:
    ```docker run --rm -v ~/dnb:/mnt --name data-extractor-1 natibenmen/dnb-data-extractor:1.0 python3 ./process.py /mnt/data_extractor.zip```

    Please note that ```<container dir>``` should not overwrite ```/opt```, where the process.py script is located.

2. Build the docker image by yourself
    
    * Pull this repository, same as above
    * On the root directory run:

       ``` docker build -t <image name> . ```
    * Run the image, same as when you pull the image from docker hub


## Software structure
### Direcotories, files and configuration
The processor.py is in the repo root dir.
The two configuration files are located at the cfg dir, both were extracted from the data_extractor.zip:
1. ```entity_mapping.tsv``` - was supplied as is

2. ```parsing_config.json``` - created manually based on the data in the ```data_extractor_processing.pdf```

I had two assumptions here:
1. I assume that the both ```entity_mapping.tsv``` and ```data_extractor_processing.pdf``` were supplied as part of the insturctions, and they will not be present in the input zip when the scrit will be run later. So I treated the ```entity_mapping.tsv``` as static configuration. If new configuration will supplied within every input file, the code can easily be adjusted.

2. The input file name within the ```data_extractor_processing.pdf``` is ```data.txt```. In that file it says that the name of the input file in the input zip is ```debts.txt```. I thouth it is a kind of error and my code assume it will only be ```data.txt```.

More assumptions:

3. Although the output example contains no spaces between key and values, I assumes it is not a requirement, so I left a space for readability.
4. When no entity mapping code is found, the 'Unknown' is applied.

### Code
The per line parsing is done by the ```LineParser``` class.
The whole file processing is done by the ```DataProcessor``` class .
For each line, ```DataProcessor``` instnsiates a new ```LineParser``` class, and then delete it to save memory. One can argue about the need to create an instance for each line, saying 
that same instance can process all lines or use a functiona approach, which will save some cpu. However the chosen approach is more modular and still not memory waster.
The configuration is generated by the ```DataProcessor``` and passed to the ```LineParser``` constructor. As Python passes function arguments by ref, it is efficient. 

#### Naive approace
The debts are collected in a dictionary where the key is the ```identification_number``` and the value is a list of ```Debts```. Each ``` debt``` is appended to the list.
The specific output format per the requiremenst is applied on print time.
I left this implementaion in the code, but only the more efficient one is actually used. 

#### More efficient approach
This requires two passed on the file. One pass create a much smaller ref dictionary with the id (=```identification_number```) as key and the value a list of reference to lines where that key is found. Then the socond pass read only the lines per the id.

Here there are two approaches as well:

1. Line index and python linecache package

This approack tracks the line numbers in the ref dictionary. The python linecache package is the used in the second pass to read the epecific lines. However, after few test and soem reading, I suspect that behind the seen the packege (as its name implies) uses a lot of memory for its cache.

2. File seek per byte count
This approach tracks the line offset in bytes within the file. The standard readline() function is used.
The advantage is that there is not cache on the expence of slower run time of about 15 percent.

Both approaches have the same problem that as the input file is larger, the ref dictionary size increases.
A mechanism of writing that data into file system, with key pineters can be created. However that is beyond this task. Or, a standard cache cluster like Redis will be used in production.

## Memory measurments
I used few technics to measure memory usage.
1. The sys.getsizeof() function gives much samller memory size than the actual one. Ror large nested objects as this script uses, the gap is in order of magnitues .
I create (based on digging the net) a function (get_actualsize) that calculate the accurate memory size of any object.
2. However, this only measure the memory size of objects used directly by the script, but still doesn't gives the memory used by python libraries.
### Get the total memeory and cpu the script uses
Using the unix top utility gives an overall visibility on the script memory usage without getting into the granularity of get_actualsize.
While running the script, I run the top utility in the backgound writing the output every second into a log file. 
The script itself writes statistics data into a log file. 

I crated a file of 8MB lines (based on the supplied 1MB lines data file). Althouth in the original one there are only a little that 30K duplication lines
and this new one is naturaly all duplication, which affects the number of keys in the ref dict, this was good for comparision between using seek and linecache. The naive implementation crashed during processing the 8MB input file.

### The results
I tested it several time with almost the same results.

* 1MB data input:
    - linecache:: memory usge: 12%, cpu: 100%,  Run time:  71 seconds
    - seek     :: memory usge:  6%, cpu: 100%,  Run time: 100 seconds

* 8MB data input:
    - linecache:: memory usge: 60%, cpu: 100%,  Run time: 252 seconds
    - seek     :: memory usge: 15%, cpu: 100%,  Run time: 318 seconds

#### Conclusions
Seek it the best apporach. It consumes the memory of the ref dictionary only, and its run time is just about 20% slower.
Obviously it has a limit when the input file increased, and potential solutions as I mentined above

## What is still missing
1. Testing on even larger input file
2. Functional tests are missing
